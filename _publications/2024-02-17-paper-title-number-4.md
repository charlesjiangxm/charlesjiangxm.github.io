---
title: "SparseNN: An energy-efficient neural network accelerator exploiting input and output sparsity. In 2018 Design, Automation & Test in Europe Conference & Exhibition (DATE). 241â€“244"
collection: publications
category: conferences
permalink: https://arxiv.org/abs/1711.01263
excerpt: 'Contemporary Deep Neural Network (DNN) contains millions of synaptic connections with tens to hundreds of layers. The large computation and memory requirements pose a challenge to the hardware design. In this work, we leverage the intrinsic activation sparsity of DNN to substantially reduce the execution cycles and the energy consumption. An end-to-end training algorithm is proposed to develop a lightweight run-time predictor for the output activation sparsity on the fly. From our experimental results, the computation overhead of the prediction phase can be reduced to less than 5% of the original feedforward phase with negligible accuracy loss. Furthermore, an energy-efficient hardware architecture, SparseNN, is proposed to exploit both the input and output sparsity. SparseNN is a scalable architecture with distributed memories and processing elements connected through a dedicated on-chip network. Compared with the state-of-the-art accelerators which only exploit the input sparsity, SparseNN can achieve a 10%-70% improvement in throughput and a power reduction of around 50%.'
date: 2017-11-03
venue: '2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)'
slidesurl: 'https://arxiv.org/abs/1711.01263'
paperurl: 'https://arxiv.org/abs/1711.01263'
citation: 'J. Zhu, J. Jiang, X. Chen and C. Tsui, "SparseNN: An energy-efficient neural network accelerator exploiting input and output sparsity," 2018 Design, Automation & Test in Europe Conference & Exhibition (DATE), Dresden, Germany, 2018, pp. 241-244, doi: 10.23919/DATE.2018.8342010.'
---

The contents above will be part of a list of publications, if the user clicks the link for the publication than the contents of section will be rendered as a full page, allowing you to provide more information about the paper for the reader. When publications are displayed as a single page, the contents of the above "citation" field will automatically be included below this section in a smaller font.